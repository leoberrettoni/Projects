{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical  # Import to_categorical directly\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lets' see which is the dimesnions of the images we have as input. \n",
    "We will take as example just the filder with pictures about the angry class'''\n",
    "\n",
    "DATASET_DIR='dataset/Angry/'\n",
    "s_dim1=1000000\n",
    "s_dim2=1000000\n",
    "for i in os.listdir(DATASET_DIR):\n",
    "    img = cv2.imread(DATASET_DIR+i, cv2.IMREAD_COLOR) # imread returns un array numpy\n",
    "    print(img.shape)\n",
    "    if img is not None:\n",
    "        if img.shape[0]<s_dim1:\n",
    "            s_dim1=img.shape[0]\n",
    "        if img.shape[1]<s_dim2:\n",
    "            s_dim2=img.shape[1]\n",
    "\n",
    "print(f'The minimum number of pixels in the long side of the image is {s_dim1}')\n",
    "print(f'The minimum number of pixels in the short side of the image is {s_dim2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the shape is not equal for all the images, this suggests us that we will need to reshape them before training the model. <br>\n",
    "Since we will need to reshape the images, if there are some iages whose dimensions are smaller than the reshaoed one, we will need to discard them so we don't risk <br>\n",
    "strange outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images before erasing: 1313\n",
      "Number of images after erasing: 1289\n"
     ]
    }
   ],
   "source": [
    "'Let\\'s say we want the size of the images to be (125x125), we will discard alal teh images where one of the two dimensions is saller than 125'\n",
    "\n",
    "count_pre_erase=0\n",
    "for i in os.listdir(DATASET_DIR):\n",
    "    count_pre_erase+=1\n",
    "\n",
    "\n",
    "for filename in os.listdir(DATASET_DIR):\n",
    "    img_path = os.path.join(DATASET_DIR, filename)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    if img is not None:\n",
    "        height, width = img.shape[:2]\n",
    "        if height < 125 or width < 125:\n",
    "            os.remove(img_path)  # Delete the image\n",
    "\n",
    "count_post_erase=0\n",
    "for i in os.listdir(DATASET_DIR):\n",
    "    count_post_erase+=1\n",
    "\n",
    "\n",
    "print(f'Number of images before erasing: {count_pre_erase}')\n",
    "print(f'Number of images after erasing: {count_post_erase}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the amount of images that we loose is not too high. <br>\n",
    "So let's suppose that this rules is valid for all the folders containing images for different emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "since augmenting the pixels in an image could end up in an undeesired result, \n",
    "we will eraase those images that have at least one of the two dimensions smaller than 125'''\n",
    "\n",
    "EMOTION_DIR = 'dataset/'\n",
    "\n",
    "# Loop over each emotion directory in the dataset folder\n",
    "for emotion in os.listdir(EMOTION_DIR):\n",
    "    folder = os.path.join(EMOTION_DIR, emotion)\n",
    "    \n",
    "    # Check if the folder variable is indeed a directory\n",
    "    if os.path.isdir(folder):\n",
    "        # Loop over each image file in the emotion directory\n",
    "        for filename in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "            \n",
    "            if img is not None:\n",
    "                height, width = img.shape[:2]\n",
    "                if height < 125 or width < 125:\n",
    "                    os.remove(img_path)\n",
    "                    print(f\"Deleted {img_path} because its size was smaller than 125.\")\n",
    "            else:\n",
    "                print(f\"Failed to read image at path: {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: dataset/Angry\n",
      "Processing directory: dataset/Happy\n",
      "Processing directory: dataset/Neutral\n",
      "Processing directory: dataset/Sad\n",
      "Processing directory: dataset/Surprise\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, UnidentifiedImageError\n",
    "import os\n",
    "\n",
    "'''\n",
    "for sake of consistency, we want all the images having the same format, \n",
    "the code below will convert all the images in the dataset to jpg format'''\n",
    "\n",
    "\n",
    "# Define the list of class directories\n",
    "classes = ['Angry', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "EMOTION_DIR = 'dataset'  # Ensure this is the correct path to your dataset\n",
    "\n",
    "# Iterate through each class directory\n",
    "for c in classes:\n",
    "    current_dir = os.path.join(EMOTION_DIR, c)\n",
    "    print(f\"Processing directory: {current_dir}\")  # Debugging output\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(current_dir):\n",
    "        print(f\"Directory not found: {current_dir}\")\n",
    "        continue\n",
    "\n",
    "    # Iterate through each file in the current directory\n",
    "    for f in os.listdir(current_dir):\n",
    "        if f.endswith('.png'):  # Process only PNG files\n",
    "            img_path = os.path.join(current_dir, f)\n",
    "\n",
    "            # Check if the file exists\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"File not found: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Open the PNG image\n",
    "                with Image.open(img_path) as png_image:\n",
    "                    # Convert image to 'RGB' if it's 'P' or 'RGBA' (JPEG does not support transparency)\n",
    "                    if png_image.mode in ('RGBA', 'LA', 'P'):\n",
    "                        png_image = png_image.convert('RGB')\n",
    "\n",
    "                    # Define the new file name with the JPEG extension\n",
    "                    jpg_file = os.path.splitext(f)[0] + '.jpg'\n",
    "                    jpg_path = os.path.join(current_dir, jpg_file)\n",
    "\n",
    "                    # Save the image in JPEG format\n",
    "                    png_image.save(jpg_path, 'JPEG')\n",
    "\n",
    "                # Remove the original PNG file\n",
    "                os.remove(img_path)\n",
    "\n",
    "            except UnidentifiedImageError as e:\n",
    "                print(f\"Error opening image {f} in {current_dir}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error processing image {f} in {current_dir}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will now implement 2 diferent approaches to solve our problem in two different ways.<br> The first one will be taking the images, opening them with openCV, flatten the associated numpy array and succesively store the flattened vectors into a csv file that will be evaluated by a classical Neural Network.<br> The second apaproach will be characterised by the use of CNN with convolution and maxpooling layers; taking as input the images as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this piece of code we will create a csv file conatining for each row the vector of pixels of the image and the label.\n",
    "We will be doing it for each of the classes we have in the dataset and finally we will print the elements we have for each class.'''\n",
    "\n",
    "EMOTION_DIR = 'dataset/'\n",
    "# set the nmae of the csv file\n",
    "OUTFILE_NAME = \"EMOTIONS.csv\"\n",
    "# set the scale of the images\n",
    "SCALE = (125, 125)\n",
    "\n",
    "# opening the csv file in the wrtie mode so that we can append the vector of pixels and the label\n",
    "out = open(EMOTION_DIR+OUTFILE_NAME, \"w\")\n",
    "# assignign to each class a number\n",
    "classes = {\"Angry\":\"1\", \"Happy\":\"0\", \"Neutral\":\"2\", \"Sad\":\"3\", \"Surprise\":\"4\"}\n",
    "# setting a counter to check how many images we have for each class\n",
    "counter = {\"Angry\":0, \"Happy\":0, \"Neutral\":0, \"Sad\":0, \"Surprise\":0}\n",
    "\n",
    "print(\"Letture di tutte le immagini da %s\" % EMOTION_DIR)\n",
    "print(\"Scrittura in %s\" % OUTFILE_NAME)\n",
    "\n",
    "for c in classes:\n",
    "    \n",
    "    current_dir = EMOTION_DIR+c\n",
    "\n",
    "    for f in os.listdir(current_dir): # iteerating over all the files that are in each of the emotion directory\n",
    "\n",
    "        img = cv2.imread(current_dir+\"/\"+f, cv2.IMREAD_GRAYSCALE) # reading the image in grayscale\n",
    "        img = cv2.resize(img, SCALE) # resize the image\n",
    "        img = img.flatten().astype(str) # trasformiamo l'immagine in un array di stringhe\n",
    "        data = \",\".join(img)+\",\"+classes[c] # scriviamo prima il valore dei vari pixel e poi il label\n",
    "        out.write(data+\"\\n\")\n",
    "        \n",
    "        counter[c]+=1\n",
    "        print(f'{c}, {counter[c]}')\n",
    "\n",
    "out.close()\n",
    "print(\"Immagini scritte: %s\" %counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have created the csv containing the values of the pixels and the label of each pictures we can import it in a dataframe\n",
    "df = pd.read_csv(EMOTION_DIR+OUTFILE_NAME, header=None) # we don't have a header in the csv file\n",
    "df.shape\n",
    "'''\n",
    "as we can see we have 14634 rows, that is the sum of each value of the counter we set in the chunk before,\n",
    " and 15626 columns that is equal to 125x125 the amount of pixels in the image, plus the column of the label'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preparing the data for the model.'''\n",
    "\n",
    "# we split the dataframe in two parts, the first one containing all the columns except the last one, that is the label.\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1:]\n",
    "# we split the dataset in train and test set, the test set will be the 30% of the dataset, we set shuffle to True so that the data will be shuffled before splitting,\n",
    "# and we set stratify to y so that the distribution of the labels in the train and test set will be the same.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=42)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train_t = encoder.fit_transform(y_train)\n",
    "y_test_t=encoder.transform(y_test)\n",
    "y_train = to_categorical(y_train_t)\n",
    "y_test = to_categorical(y_test_t)\n",
    "\n",
    "# we finally divide the values by 255 so that we have all the values between 0 and 1, we normalize them.\n",
    "X_train/=255\n",
    "X_test/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "now that the preprocessing part has been finished we can proceed with the traing of the model. We will set and train a classical neural netowrk.'''\n",
    "\n",
    "# Creiamo la Rete Neurale\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(5121, activation='relu', input_dim=X_train.shape[1]))\n",
    "model1.add(Dropout(0.3)) # Dropout to avoid overfitting\n",
    "model1.add(Dense(2570, activation='relu'))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(2570, activation='relu'))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(1285, activation='relu'))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(1285, activation='relu'))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model1.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model1.fit(X_train, y_train, epochs=200, batch_size=100)\n",
    "\n",
    "# Evaluating the model\n",
    "metrics_train1 = model1.evaluate(X_train, y_train, verbose=0)\n",
    "metrics_test1 = model1.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Train Accuracy = %.4f - Train Loss = %.4f\" % (metrics_train1[1], metrics_train1[0]))\n",
    "print(\"Test Accuracy = %.4f - Test Loss = %.4f\" % (metrics_test1[1], metrics_test1[0]))\n",
    "\n",
    "# Saving the model\n",
    "model1.save(\"emotions_ANN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "'''\n",
    "We have some classes that have a number of images that is much smaller than the others... too small to fit a good multiclass classification model.\n",
    "So aim so is the one of augmenting the images of the less populated classes by rotating, flipping and zooming the images.\n",
    "The very last step will be the one of merging the augmented images with the original ones and store them in a new folder.'''\n",
    "\n",
    "\n",
    "\n",
    "def merge_directories(sources, destination):\n",
    "    \"\"\"\n",
    "    Merge multiple source directories into a single destination directory.\n",
    "    Files from source directories are copied into the destination directory.\n",
    "    Note: This may overwrite files in the destination if filenames conflict.\n",
    "    \n",
    "    Parameters:\n",
    "    - sources: List of paths to source directories.\n",
    "    - destination: Path to the destination directory.\n",
    "    \"\"\"\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    # Copy files from each source directory to the destination\n",
    "    for source in sources:\n",
    "        for filename in os.listdir(source):\n",
    "            source_file = os.path.join(source, filename)\n",
    "            if os.path.isfile(source_file):  # Ensure it's a file\n",
    "                shutil.copy2(source_file, destination)  # This may overwrite files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import os\n",
    "\n",
    "# set the categories we want to augment\n",
    "categories = ['Angry', 'Surprise']\n",
    "# set by how much we want to augment the images\n",
    "categories_dict={'Angry':2, 'Surprise':3}\n",
    "\n",
    "\n",
    "'''\n",
    "we will now merge the two directories (for each of the augmented emotion) in a single one, \n",
    "after performing this we can copy the remaining three folder in the final_folders directory'''\n",
    "\n",
    "for category in categories:\n",
    "    # Example for a single category that needs augmentation\n",
    "    category = category\n",
    "    source_directory = os.path.join('dataset', category)\n",
    "    augmented_directory = os.path.join('dataset/augmented_train', category)\n",
    "    os.makedirs(augmented_directory, exist_ok=True)  # Ensure the target directory exists\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    # Assume you've determined each image needs to be augmented 5 times\n",
    "    augment_factor = categories_dict[category]\n",
    "\n",
    "    for filename in os.listdir(source_directory):\n",
    "        if filename.endswith(\".jpg\"):  # Ensure to process only jpg images or adjust as needed\n",
    "            img_path = os.path.join(source_directory, filename)\n",
    "            img = load_img(img_path)\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "\n",
    "            # Generate and save augmented images\n",
    "            i = 0\n",
    "            for batch in datagen.flow(x, batch_size=1, save_to_dir=augmented_directory, save_prefix='aug', save_format='jpg'):\n",
    "                i += 1\n",
    "                if i >= augment_factor:\n",
    "                    break  # Stop after generating the desired number of augmented images\n",
    "    \n",
    "    # usage\n",
    "    sources_1=os.path.join('dataset', category)\n",
    "    sources_2=os.path.join('dataset/augmented_train', category)\n",
    "    sources = [sources_1, sources_2]\n",
    "    destination = os.path.join('dataset/final_folders', category)\n",
    "    merge_directories(sources, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Angry': 3415, 'Happy': 3402, 'Neutral': 3941, 'Sad': 3858, 'Surprise': 3649}\n"
     ]
    }
   ],
   "source": [
    "''' now for seek of clarity let's count the elements of each folder'''\n",
    "# counting elements in each folder containied in the final folders directory\n",
    "counter = {\"Angry\":0, \"Happy\":0, \"Neutral\":0, \"Sad\":0, \"Surprise\":0}\n",
    "\n",
    "base_dir = \"dataset/final_folders\"\n",
    "for elem in os.listdir(base_dir):\n",
    "    path = os.path.join(base_dir, elem)\n",
    "    if os.path.isdir(path):  # Check if it's a directory\n",
    "        for f in os.listdir(path):\n",
    "            counter[elem] += 1\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12784 images belonging to 5 classes.\n",
      "Found 5477 images belonging to 5 classes.\n",
      "{'Angry': 0, 'Happy': 1, 'Neutral': 2, 'Sad': 3, 'Surprise': 4}\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 55s 847ms/step - loss: 1.6095 - accuracy: 0.2295 - f1_m: 4.4753e-04 - precision_m: 0.0179 - recall_m: 2.3437e-04\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 56s 861ms/step - loss: 1.4797 - accuracy: 0.3561 - f1_m: 0.0691 - precision_m: 0.5389 - recall_m: 0.0383\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 57s 886ms/step - loss: 1.3584 - accuracy: 0.4280 - f1_m: 0.2072 - precision_m: 0.6228 - recall_m: 0.1259\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 58s 896ms/step - loss: 1.2837 - accuracy: 0.4699 - f1_m: 0.2977 - precision_m: 0.6545 - recall_m: 0.1937\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 59s 907ms/step - loss: 1.2225 - accuracy: 0.4985 - f1_m: 0.3617 - precision_m: 0.6836 - recall_m: 0.2472\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 59s 916ms/step - loss: 1.1789 - accuracy: 0.5265 - f1_m: 0.3933 - precision_m: 0.6860 - recall_m: 0.2768\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 59s 917ms/step - loss: 1.1320 - accuracy: 0.5444 - f1_m: 0.4363 - precision_m: 0.7010 - recall_m: 0.3177\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 60s 923ms/step - loss: 1.1056 - accuracy: 0.5573 - f1_m: 0.4579 - precision_m: 0.7057 - recall_m: 0.3405\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 61s 945ms/step - loss: 1.0811 - accuracy: 0.5675 - f1_m: 0.4793 - precision_m: 0.7110 - recall_m: 0.3632\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 62s 958ms/step - loss: 1.0509 - accuracy: 0.5758 - f1_m: 0.4981 - precision_m: 0.7141 - recall_m: 0.3835\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 62s 956ms/step - loss: 1.0174 - accuracy: 0.5919 - f1_m: 0.5224 - precision_m: 0.7200 - recall_m: 0.4106\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 62s 962ms/step - loss: 1.0082 - accuracy: 0.5936 - f1_m: 0.5259 - precision_m: 0.7257 - recall_m: 0.4134\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 62s 958ms/step - loss: 0.9941 - accuracy: 0.6002 - f1_m: 0.5347 - precision_m: 0.7260 - recall_m: 0.4247\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 62s 954ms/step - loss: 0.9552 - accuracy: 0.6134 - f1_m: 0.5623 - precision_m: 0.7335 - recall_m: 0.4566\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 62s 956ms/step - loss: 0.9489 - accuracy: 0.6205 - f1_m: 0.5652 - precision_m: 0.7324 - recall_m: 0.4607\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 62s 960ms/step - loss: 0.9173 - accuracy: 0.6340 - f1_m: 0.5819 - precision_m: 0.7416 - recall_m: 0.4796\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 62s 957ms/step - loss: 0.9104 - accuracy: 0.6380 - f1_m: 0.5948 - precision_m: 0.7450 - recall_m: 0.4956\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 62s 954ms/step - loss: 0.8942 - accuracy: 0.6462 - f1_m: 0.5964 - precision_m: 0.7433 - recall_m: 0.4988\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 62s 955ms/step - loss: 0.8855 - accuracy: 0.6500 - f1_m: 0.6127 - precision_m: 0.7509 - recall_m: 0.5183\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 62s 960ms/step - loss: 0.8729 - accuracy: 0.6529 - f1_m: 0.6133 - precision_m: 0.7496 - recall_m: 0.5197\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 156s 2s/step - loss: 0.8576 - accuracy: 0.6555 - f1_m: 0.6246 - precision_m: 0.7592 - recall_m: 0.5312\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 120s 2s/step - loss: 0.8513 - accuracy: 0.6613 - f1_m: 0.6248 - precision_m: 0.7554 - recall_m: 0.5334\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 111s 2s/step - loss: 0.8407 - accuracy: 0.6643 - f1_m: 0.6294 - precision_m: 0.7538 - recall_m: 0.5409\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 136s 2s/step - loss: 0.8317 - accuracy: 0.6683 - f1_m: 0.6367 - precision_m: 0.7610 - recall_m: 0.5480\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 111s 2s/step - loss: 0.8218 - accuracy: 0.6697 - f1_m: 0.6369 - precision_m: 0.7586 - recall_m: 0.5494\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 375s 6s/step - loss: 0.8061 - accuracy: 0.6810 - f1_m: 0.6536 - precision_m: 0.7656 - recall_m: 0.5708\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 77s 1s/step - loss: 0.7982 - accuracy: 0.6809 - f1_m: 0.6553 - precision_m: 0.7687 - recall_m: 0.5716\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 75s 1s/step - loss: 0.8094 - accuracy: 0.6809 - f1_m: 0.6481 - precision_m: 0.7625 - recall_m: 0.5641\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 75s 1s/step - loss: 0.7894 - accuracy: 0.6895 - f1_m: 0.6577 - precision_m: 0.7692 - recall_m: 0.5750\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 75s 1s/step - loss: 0.7835 - accuracy: 0.6897 - f1_m: 0.6642 - precision_m: 0.7708 - recall_m: 0.5842\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 71s 1s/step - loss: 0.7703 - accuracy: 0.6977 - f1_m: 0.6685 - precision_m: 0.7755 - recall_m: 0.5880\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 71s 1s/step - loss: 0.7600 - accuracy: 0.6956 - f1_m: 0.6722 - precision_m: 0.7742 - recall_m: 0.5945\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 74s 1s/step - loss: 0.7595 - accuracy: 0.7017 - f1_m: 0.6750 - precision_m: 0.7776 - recall_m: 0.5969\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 71s 1s/step - loss: 0.7592 - accuracy: 0.6938 - f1_m: 0.6758 - precision_m: 0.7795 - recall_m: 0.5969\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.7565 - accuracy: 0.6964 - f1_m: 0.6744 - precision_m: 0.7720 - recall_m: 0.5992\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.7445 - accuracy: 0.7058 - f1_m: 0.6831 - precision_m: 0.7794 - recall_m: 0.6084\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.7393 - accuracy: 0.7072 - f1_m: 0.6832 - precision_m: 0.7796 - recall_m: 0.6086\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.7323 - accuracy: 0.7126 - f1_m: 0.6909 - precision_m: 0.7871 - recall_m: 0.6161\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.7228 - accuracy: 0.7109 - f1_m: 0.6941 - precision_m: 0.7836 - recall_m: 0.6236\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.7179 - accuracy: 0.7171 - f1_m: 0.6939 - precision_m: 0.7824 - recall_m: 0.6238\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 73s 1s/step - loss: 0.7077 - accuracy: 0.7241 - f1_m: 0.7013 - precision_m: 0.7849 - recall_m: 0.6340\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.7064 - accuracy: 0.7208 - f1_m: 0.7001 - precision_m: 0.7887 - recall_m: 0.6297\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.6939 - accuracy: 0.7254 - f1_m: 0.7090 - precision_m: 0.7929 - recall_m: 0.6416\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.6961 - accuracy: 0.7240 - f1_m: 0.7065 - precision_m: 0.7950 - recall_m: 0.6362\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.6900 - accuracy: 0.7279 - f1_m: 0.7099 - precision_m: 0.7929 - recall_m: 0.6432\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 73s 1s/step - loss: 0.6953 - accuracy: 0.7241 - f1_m: 0.7049 - precision_m: 0.7889 - recall_m: 0.6374\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 72s 1s/step - loss: 0.6948 - accuracy: 0.7268 - f1_m: 0.7044 - precision_m: 0.7894 - recall_m: 0.6364\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 73s 1s/step - loss: 0.6793 - accuracy: 0.7365 - f1_m: 0.7203 - precision_m: 0.8019 - recall_m: 0.6541\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 73s 1s/step - loss: 0.6737 - accuracy: 0.7348 - f1_m: 0.7157 - precision_m: 0.7934 - recall_m: 0.6522\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 74s 1s/step - loss: 0.6757 - accuracy: 0.7346 - f1_m: 0.7188 - precision_m: 0.7997 - recall_m: 0.6532\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 74s 1s/step - loss: 0.6579 - accuracy: 0.7384 - f1_m: 0.7300 - precision_m: 0.8053 - recall_m: 0.6679\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 73s 1s/step - loss: 0.6630 - accuracy: 0.7374 - f1_m: 0.7219 - precision_m: 0.7964 - recall_m: 0.6606\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 59s 912ms/step - loss: 0.6705 - accuracy: 0.7363 - f1_m: 0.7191 - precision_m: 0.7973 - recall_m: 0.6554\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 57s 878ms/step - loss: 0.6572 - accuracy: 0.7434 - f1_m: 0.7291 - precision_m: 0.8008 - recall_m: 0.6695\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 58s 892ms/step - loss: 0.6571 - accuracy: 0.7379 - f1_m: 0.7246 - precision_m: 0.7983 - recall_m: 0.6637\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 58s 894ms/step - loss: 0.6499 - accuracy: 0.7426 - f1_m: 0.7314 - precision_m: 0.8039 - recall_m: 0.6711\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 59s 914ms/step - loss: 0.6505 - accuracy: 0.7444 - f1_m: 0.7337 - precision_m: 0.8045 - recall_m: 0.6747\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 60s 929ms/step - loss: 0.6507 - accuracy: 0.7412 - f1_m: 0.7293 - precision_m: 0.8002 - recall_m: 0.6703\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 62s 954ms/step - loss: 0.6300 - accuracy: 0.7530 - f1_m: 0.7419 - precision_m: 0.8101 - recall_m: 0.6847\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 62s 960ms/step - loss: 0.6436 - accuracy: 0.7500 - f1_m: 0.7334 - precision_m: 0.8048 - recall_m: 0.6740\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 61s 953ms/step - loss: 0.6283 - accuracy: 0.7534 - f1_m: 0.7402 - precision_m: 0.8059 - recall_m: 0.6848\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 62s 959ms/step - loss: 0.6376 - accuracy: 0.7481 - f1_m: 0.7379 - precision_m: 0.8030 - recall_m: 0.6828\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 63s 978ms/step - loss: 0.6335 - accuracy: 0.7492 - f1_m: 0.7386 - precision_m: 0.8044 - recall_m: 0.6831\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 66s 1s/step - loss: 0.6313 - accuracy: 0.7528 - f1_m: 0.7453 - precision_m: 0.8115 - recall_m: 0.6894\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 64s 995ms/step - loss: 0.6259 - accuracy: 0.7536 - f1_m: 0.7412 - precision_m: 0.8062 - recall_m: 0.6861\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 64s 986ms/step - loss: 0.6136 - accuracy: 0.7599 - f1_m: 0.7487 - precision_m: 0.8116 - recall_m: 0.6951\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 65s 1s/step - loss: 0.6224 - accuracy: 0.7535 - f1_m: 0.7442 - precision_m: 0.8060 - recall_m: 0.6915\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 64s 996ms/step - loss: 0.6161 - accuracy: 0.7554 - f1_m: 0.7453 - precision_m: 0.8069 - recall_m: 0.6926\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 64s 995ms/step - loss: 0.6146 - accuracy: 0.7572 - f1_m: 0.7479 - precision_m: 0.8110 - recall_m: 0.6943\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 64s 998ms/step - loss: 0.6106 - accuracy: 0.7610 - f1_m: 0.7504 - precision_m: 0.8127 - recall_m: 0.6972\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 64s 996ms/step - loss: 0.6121 - accuracy: 0.7638 - f1_m: 0.7512 - precision_m: 0.8149 - recall_m: 0.6970\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 65s 1s/step - loss: 0.6184 - accuracy: 0.7584 - f1_m: 0.7471 - precision_m: 0.8127 - recall_m: 0.6917\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 64s 998ms/step - loss: 0.6019 - accuracy: 0.7654 - f1_m: 0.7553 - precision_m: 0.8136 - recall_m: 0.7052\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 64s 1000ms/step - loss: 0.6070 - accuracy: 0.7617 - f1_m: 0.7502 - precision_m: 0.8096 - recall_m: 0.6993\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 65s 1s/step - loss: 0.5949 - accuracy: 0.7682 - f1_m: 0.7593 - precision_m: 0.8199 - recall_m: 0.7073\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 65s 1s/step - loss: 0.5980 - accuracy: 0.7660 - f1_m: 0.7569 - precision_m: 0.8164 - recall_m: 0.7058\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 64s 990ms/step - loss: 0.5848 - accuracy: 0.7685 - f1_m: 0.7580 - precision_m: 0.8159 - recall_m: 0.7080\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 64s 992ms/step - loss: 0.5874 - accuracy: 0.7697 - f1_m: 0.7618 - precision_m: 0.8210 - recall_m: 0.7109\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 64s 995ms/step - loss: 0.5900 - accuracy: 0.7703 - f1_m: 0.7582 - precision_m: 0.8148 - recall_m: 0.7091\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 65s 1s/step - loss: 0.5952 - accuracy: 0.7683 - f1_m: 0.7582 - precision_m: 0.8141 - recall_m: 0.7097\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 64s 998ms/step - loss: 0.5841 - accuracy: 0.7769 - f1_m: 0.7668 - precision_m: 0.8228 - recall_m: 0.7182\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 64s 993ms/step - loss: 0.5778 - accuracy: 0.7724 - f1_m: 0.7626 - precision_m: 0.8166 - recall_m: 0.7155\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 65s 1s/step - loss: 0.5798 - accuracy: 0.7729 - f1_m: 0.7636 - precision_m: 0.8172 - recall_m: 0.7169\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 64s 999ms/step - loss: 0.5847 - accuracy: 0.7682 - f1_m: 0.7597 - precision_m: 0.8154 - recall_m: 0.7114\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 64s 996ms/step - loss: 0.5835 - accuracy: 0.7718 - f1_m: 0.7633 - precision_m: 0.8173 - recall_m: 0.7163\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 64s 983ms/step - loss: 0.5743 - accuracy: 0.7783 - f1_m: 0.7664 - precision_m: 0.8204 - recall_m: 0.7193\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 64s 989ms/step - loss: 0.5808 - accuracy: 0.7730 - f1_m: 0.7638 - precision_m: 0.8206 - recall_m: 0.7146\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 63s 985ms/step - loss: 0.5724 - accuracy: 0.7753 - f1_m: 0.7660 - precision_m: 0.8192 - recall_m: 0.7195\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 63s 983ms/step - loss: 0.5626 - accuracy: 0.7790 - f1_m: 0.7719 - precision_m: 0.8228 - recall_m: 0.7271\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 63s 983ms/step - loss: 0.5746 - accuracy: 0.7742 - f1_m: 0.7636 - precision_m: 0.8188 - recall_m: 0.7154\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 64s 988ms/step - loss: 0.5670 - accuracy: 0.7760 - f1_m: 0.7691 - precision_m: 0.8204 - recall_m: 0.7241\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 63s 983ms/step - loss: 0.5694 - accuracy: 0.7778 - f1_m: 0.7710 - precision_m: 0.8239 - recall_m: 0.7248\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 63s 985ms/step - loss: 0.5669 - accuracy: 0.7814 - f1_m: 0.7703 - precision_m: 0.8227 - recall_m: 0.7245\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 64s 985ms/step - loss: 0.5585 - accuracy: 0.7824 - f1_m: 0.7746 - precision_m: 0.8268 - recall_m: 0.7287\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 63s 980ms/step - loss: 0.5638 - accuracy: 0.7750 - f1_m: 0.7684 - precision_m: 0.8183 - recall_m: 0.7244\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 63s 979ms/step - loss: 0.5581 - accuracy: 0.7826 - f1_m: 0.7746 - precision_m: 0.8245 - recall_m: 0.7308\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 63s 982ms/step - loss: 0.5526 - accuracy: 0.7840 - f1_m: 0.7762 - precision_m: 0.8275 - recall_m: 0.7311\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 64s 990ms/step - loss: 0.5479 - accuracy: 0.7833 - f1_m: 0.7798 - precision_m: 0.8263 - recall_m: 0.7384\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 63s 982ms/step - loss: 0.5562 - accuracy: 0.7817 - f1_m: 0.7734 - precision_m: 0.8236 - recall_m: 0.7292\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 64s 988ms/step - loss: 0.5665 - accuracy: 0.7785 - f1_m: 0.7712 - precision_m: 0.8240 - recall_m: 0.7251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x11199bdf0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Once everything has been set correctly we can proceed with the training of the model.\n",
    "We will use a CNN model, ImageDataGenerator to load the images from the directory and the flow_from_directory method to load the images from the directory.'''\n",
    "\n",
    "DATASET_DIR = \"dataset/final_folders\"\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "# since we have a quite consistent number of images we cannot load them all in memory, \n",
    "# so we will use the flow_from_directory method to load the images from the directory.\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        validation_split = 0.3,\n",
    "        rescale = 1./255,\n",
    "        horizontal_flip = True, \n",
    "        zoom_range = 0.2,\n",
    "        brightness_range = [1,2]\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory( # flow from directory reads the images from the directory and returns a batch of images\n",
    "        DATASET_DIR,\n",
    "        target_size = (125, 125), # resize the images to 125x125\n",
    "        batch_size = BATCH_SIZE,\n",
    "        class_mode = \"categorical\",\n",
    "        subset = \"training\"\n",
    ")\n",
    "\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        DATASET_DIR,\n",
    "        target_size = (125, 125),\n",
    "        batch_size = BATCH_SIZE,\n",
    "        class_mode = \"categorical\",\n",
    "        subset = \"validation\",\n",
    "        shuffle=False\n",
    ")\n",
    "\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "# since can be usefull to have paramters different from the accuracy to evaluate the model we will define them here\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "# let's implement the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=4, padding=\"same\", activation=\"relu\", input_shape=(125, 125, 3)))\n",
    "model.add(MaxPooling2D(pool_size=4, strides=4))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv2D(filters=32, kernel_size=4, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=4, strides=4))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", f1_m, precision_m, recall_m])\n",
    "model.fit(train_generator, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 47s 726ms/step - loss: 0.3617 - accuracy: 0.8816 - f1_m: 0.8668 - precision_m: 0.9164 - recall_m: 0.8225\n",
      "28/28 [==============================] - 23s 809ms/step - loss: 0.5090 - accuracy: 0.8001 - f1_m: 0.7903 - precision_m: 0.8416 - recall_m: 0.7458\n",
      "Train Accuracy = 0.8816 - Train Loss = 0.3617 - Train F1= 0.8668 - Train precision= 0.9164 - Train recall= 0.8225\n",
      "Test Accuracy = 0.8001 - Test Loss = 0.5090- Test F1= 0.7903 - Test precision= 0.8416 - Test recall= 0.7458\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "metrics_train = model.evaluate(train_generator)\n",
    "metrics_test = model.evaluate(test_generator)\n",
    "\n",
    "print(\"Train Accuracy = %.4f - Train Loss = %.4f - Train F1= %.4f - Train precision= %.4f - Train recall= %.4f\" % (metrics_train[1], metrics_train[0], metrics_train[2], metrics_train[3], metrics_train[4]))\n",
    "print(\"Test Accuracy = %.4f - Test Loss = %.4f- Test F1= %.4f - Test precision= %.4f - Test recall= %.4f\" % (metrics_test[1], metrics_test[0], metrics_test[2], metrics_test[3], metrics_test[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 22s 786ms/step\n",
      "Calculated Accuracy: 0.8008\n",
      "[[907  16  39  50  12]\n",
      " [ 20 899  48  45   8]\n",
      " [ 39  50 846 239   8]\n",
      " [ 76  66 268 740   7]\n",
      " [ 26  18  34  22 994]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.85      0.89      0.87      1024\n",
      "       Happy       0.86      0.88      0.87      1020\n",
      "     Neutral       0.69      0.72      0.70      1182\n",
      "         Sad       0.68      0.64      0.66      1157\n",
      "    Surprise       0.97      0.91      0.94      1094\n",
      "\n",
      "    accuracy                           0.80      5477\n",
      "   macro avg       0.81      0.81      0.81      5477\n",
      "weighted avg       0.80      0.80      0.80      5477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Ensure the test_generator is not shuffling data\n",
    "# test_generator.shuffle = False\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(test_generator, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Extract true labels from the generator\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Calculate accuracy (optional, for verification)\n",
    "accuracy = np.mean(y_pred_bool == y_true)\n",
    "print(f\"Calculated Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate a confusion matrix (optional, for further verification)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred_bool)\n",
    "print(cm)\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_true, y_pred_bool, target_names=test_generator.class_indices.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/miniconda3/envs/tf_macos/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# let's save the model\n",
    "model.save(\"model_CNN_final.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "SCALE = (125, 125)\n",
    "## since our model uses some metrics that are not build by default in keras, we need to redefine them before loading the model\n",
    "\n",
    "# Define custom metrics\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "# Load the model with custom metrics\n",
    "custom_objects = {'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m}\n",
    "model = load_model('model_CNN_final.h5', custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is already updated for multiclass classification with 6 output units using softmax activation\n",
    "# AOpen the webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "if(not cap.read()[0]):\n",
    "    print(\"Webcam non è disponible\")\n",
    "    exit(0)\n",
    "\n",
    "# Frame just the faces using the hardcascade calssifier\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Define class names based on your classes. Adjust these names as per your actual classes.\n",
    "class_names = [\"Angry\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = face_cascade.detectMultiScale(gray, 1.1, 15)\n",
    "\n",
    "    for rect in rects:\n",
    "        img = frame[rect[1]:rect[1]+rect[3], rect[0]:rect[0]+rect[2]]\n",
    "        small_img = cv2.resize(img, SCALE)\n",
    "        small_img = cv2.cvtColor(small_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        x = small_img.astype(float) / 255.\n",
    "        x = x.reshape(1, SCALE[0], SCALE[1], 3)  # Reshape for the model\n",
    "\n",
    "        predictions = model.predict(x)\n",
    "        predicted_class_index = np.argmax(predictions)  # Get the index of the highest probability\n",
    "        predicted_class_name = class_names[predicted_class_index]  # Map index to class name\n",
    "        probability = round(np.max(predictions) * 100, 1)  # Get the highest probability\n",
    "\n",
    "        cv2.rectangle(frame, (rect[0], rect[1]), (rect[0]+rect[2], rect[1]+rect[3]), (0,255,0), 2)\n",
    "        cv2.rectangle(frame, (rect[0], rect[1]-30), (rect[0]+190, rect[1]), (0,255,0), cv2.FILLED)\n",
    "        cv2.putText(frame, predicted_class_name+\" (\"+str(probability)+\"%)\", (rect[0]+7, rect[1]-2), cv2.FONT_HERSHEY_PLAIN, 1.4, (255,255,255), 2)\n",
    "\n",
    "    cv2.imshow(\"Classification\", frame)\n",
    "\n",
    "    # press q to exit and end the session\n",
    "    if(cv2.waitKey(1) == ord(\"q\")):\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_macos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
